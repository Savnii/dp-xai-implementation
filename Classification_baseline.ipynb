{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1287a80-2245-48ce-a3ba-3f2129186758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "readmitted\n",
      "NO     54864\n",
      ">30    35545\n",
      "<30    11357\n",
      "Name: count, dtype: int64\n",
      "\n",
      "New class distribution (encoded):\n",
      "readmitted\n",
      "1    35545\n",
      "0    27432\n",
      "2    27432\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Training Target Model ---\n",
      "Epoch 10/50, Loss: 0.9225\n",
      "Epoch 20/50, Loss: 0.9118\n",
      "Epoch 30/50, Loss: 0.9087\n",
      "Epoch 40/50, Loss: 0.9060\n",
      "Epoch 50/50, Loss: 0.9040\n",
      "\n",
      "--- Evaluating Target Model Utility ---\n",
      "\n",
      "---------------------------------------------\n",
      "###Model Evaluation Metrics (Baseline) ###\n",
      "\n",
      "**Accuracy:** 0.5547\n",
      "\n",
      "**Confusion Matrix:**\n",
      "[[2467 2369  651]\n",
      " [ 277 5549 1283]\n",
      " [ 222 3250 2014]]\n",
      "\n",
      "**Classification Report (Precision, Recall, F1-Score):**\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         <30       0.83      0.45      0.58      5487\n",
      "         >30       0.50      0.78      0.61      7109\n",
      "          NO       0.51      0.37      0.43      5486\n",
      "\n",
      "    accuracy                           0.55     18082\n",
      "   macro avg       0.61      0.53      0.54     18082\n",
      "weighted avg       0.60      0.55      0.55     18082\n",
      "\n",
      "\n",
      "**Privacy Metrics:**\n",
      "  - Epsilon: Not Applicable (Baseline Model)\n",
      "  - Delta:   Not Applicable (Baseline Model)\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "Trained model saved to baseline_target_model_copy.pth\n",
      "\n",
      "Step 5: Generating SHAP explanations for Baseline model.\n",
      "\n",
      "### SHAP Feature Importance (Baseline) ###\n",
      "\n",
      "Top 10 features by mean absolute SHAP value:\n",
      "payer_code_FR: 9.0088\n",
      "insulin_No: 1.4022\n",
      "change_No: 1.2745\n",
      "glipizide_No: 1.0133\n",
      "insulin_Steady: 0.5691\n",
      "metformin_No: 0.4134\n",
      "discharge_disposition_id: 0.3035\n",
      "num_medications: 0.2954\n",
      "rosiglitazone_No: 0.2507\n",
      "age_[50-60): 0.2344\n",
      "\n",
      "Saving SHAP results.\n",
      "Saved SHAP feature importance to 'shap_feature_importance_baseline_copy.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap \n",
    "import os\n",
    "\n",
    "df = pd.read_csv(\"datasets/diabetic_data.csv\")\n",
    "target_col = \"readmitted\"\n",
    "\n",
    "X = df.drop(columns=[\"encounter_id\", \"patient_nbr\", target_col])\n",
    "y = df[target_col]\n",
    "X.drop(columns=['diag_1', 'diag_2', 'diag_3', 'medical_specialty', 'citoglipton', 'glimepiride-pioglitazone'], inplace=True, errors='ignore')\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "for col in categorical_cols: X[col] = X[col].astype(str)\n",
    "\n",
    "X_encoded = X.copy()\n",
    "encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_encoded[col] = le.fit_transform(X_encoded[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "print(\"Original class distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "target_size_after_undersampling = 27432\n",
    "under_strategy = {'NO': target_size_after_undersampling}\n",
    "over_strategy = {\"<30\": target_size_after_undersampling}\n",
    "under = RandomUnderSampler(sampling_strategy=under_strategy, random_state=42)\n",
    "over = SMOTE(sampling_strategy=over_strategy, random_state=42, k_neighbors=5)\n",
    "pipeline = Pipeline([(\"under\", under), (\"over\", over)])\n",
    "X_resampled_num, y_resampled = pipeline.fit_resample(X_encoded, y)\n",
    "\n",
    "X_resampled_decoded = X_resampled_num.copy()\n",
    "for col, le in encoders.items():\n",
    "    X_resampled_decoded[col] = le.inverse_transform(X_resampled_num[col].astype(int))\n",
    "\n",
    "X_resampled_ohe = pd.get_dummies(X_resampled_decoded, drop_first=True)\n",
    "target_mapping = {'<30': 0, '>30': 1, 'NO': 2}\n",
    "y_resampled_encoded = y_resampled.map(target_mapping)\n",
    "balanced_df = X_resampled_ohe.copy()\n",
    "balanced_df[target_col] = y_resampled_encoded\n",
    "feature_names = X_resampled_ohe.columns.tolist()\n",
    "\n",
    "print(\"\\nNew class distribution (encoded):\")\n",
    "print(y_resampled_encoded.value_counts())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled_ohe, y_resampled_encoded, test_size=0.2, random_state=42, stratify=y_resampled_encoded\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "test_ds = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_ds, batch_size=128)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "num_classes = len(y_resampled_encoded.unique())\n",
    "\n",
    "class MulticlassNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(64, 32), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "print(\"\\n--- Training Target Model ---\")\n",
    "baseline_model = MulticlassNN(input_dim, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(baseline_model.parameters(), lr=1e-3)\n",
    "epochs = 50 \n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    baseline_model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = baseline_model(xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"\\n--- Evaluating Target Model Utility ---\")\n",
    "baseline_model.eval()\n",
    "all_preds, all_true = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        outputs = baseline_model(xb)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_true.extend(yb.cpu().numpy())\n",
    "\n",
    "# --- 2. Explicit Metrics Reporting ---\n",
    "accuracy = accuracy_score(all_true, all_preds)\n",
    "report = classification_report(all_true, all_preds, target_names=['<30', '>30', 'NO'])\n",
    "cm = confusion_matrix(all_true, all_preds)\n",
    "\n",
    "print(\"\\n\" + \"---\" * 15)\n",
    "print(\"###Model Evaluation Metrics (Baseline) ###\")\n",
    "print(f\"\\n**Accuracy:** {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n**Confusion Matrix:**\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\n**Classification Report (Precision, Recall, F1-Score):**\")\n",
    "print(report)\n",
    "\n",
    "print(\"\\n**Privacy Metrics:**\")\n",
    "print(\"  - Epsilon: Not Applicable (Baseline Model)\")\n",
    "print(\"  - Delta:   Not Applicable (Baseline Model)\")\n",
    "print(\"---\" * 15 + \"\\n\")\n",
    "\n",
    "MODEL_PATH = \"baseline_target_model_copy.pth\"\n",
    "torch.save(baseline_model.state_dict(), MODEL_PATH)\n",
    "print(f\"\\nTrained model saved to {MODEL_PATH}\")\n",
    "\n",
    "print(\"\\nStep 5: Generating SHAP explanations for Baseline model.\")\n",
    "\n",
    "clean_model = MulticlassNN(input_dim, num_classes).to(device)\n",
    "clean_model.load_state_dict(baseline_model.state_dict())\n",
    "clean_model.eval()\n",
    "\n",
    "background_size = min(100, len(X_train_tensor))\n",
    "background_data = X_train_tensor[np.random.choice(len(X_train_tensor), background_size, replace=False)].to(device)\n",
    "X_explain = X_test_tensor[:20].to(device)\n",
    "\n",
    "explainer = shap.DeepExplainer(clean_model, background_data)\n",
    "shap_values_list = explainer.shap_values(X_explain)\n",
    "\n",
    "mean_abs_shap = np.mean(np.abs(np.array(shap_values_list)), axis=(0, 2))\n",
    "feature_names = X_train.columns.tolist()\n",
    "feature_importance = dict(zip(feature_names, mean_abs_shap))\n",
    "\n",
    "print(\"\\n### SHAP Feature Importance (Baseline) ###\")\n",
    "print(\"\\nTop 10 features by mean absolute SHAP value:\")\n",
    "sorted_importance = sorted(feature_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "for feature, value in sorted_importance[:10]:\n",
    "    print(f\"{feature}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nSaving SHAP results.\")\n",
    "\n",
    "shap_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'mean_abs_shap': mean_abs_shap\n",
    "}).sort_values('mean_abs_shap', ascending=False)\n",
    "\n",
    "shap_df.to_csv('shap_feature_importance_baseline_copy.csv', index=False)\n",
    "print(\"Saved SHAP feature importance to 'shap_feature_importance_baseline_copy.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226ec859-4632-4b05-9aef-660402bc6f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2240/450248205.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  baseline_model.load_state_dict(torch.load(MODEL_PATH))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Creating attack dataset for the Baseline Model.\n",
      "\n",
      "Step 2: Running 10 MIA trials on the Baseline Model...\n",
      "  Trial 1/10 (Seed: 42) -> MIA Advantage: 0.0033\n",
      "  Trial 2/10 (Seed: 43) -> MIA Advantage: -0.0018\n",
      "  Trial 3/10 (Seed: 44) -> MIA Advantage: 0.0089\n",
      "  Trial 4/10 (Seed: 45) -> MIA Advantage: -0.0009\n",
      "  Trial 5/10 (Seed: 46) -> MIA Advantage: -0.0155\n",
      "  Trial 6/10 (Seed: 47) -> MIA Advantage: -0.0073\n",
      "  Trial 7/10 (Seed: 48) -> MIA Advantage: 0.0099\n",
      "  Trial 8/10 (Seed: 49) -> MIA Advantage: -0.0048\n",
      "  Trial 9/10 (Seed: 50) -> MIA Advantage: -0.0057\n",
      "  Trial 10/10 (Seed: 51) -> MIA Advantage: -0.0033\n",
      "\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "Final Robust MIA Results for Baseline Model\n",
      "  Mean MIA Advantage: -0.0017\n",
      "  Std Dev of MIA Advantage: 0.0072\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "\n",
      "Successfully executed weighted MIA. Mean and Std Dev saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "num_classes = len(torch.unique(y_train_tensor))\n",
    "MODEL_PATH = \"baseline_target_model_copy.pth\"\n",
    "\n",
    "class MulticlassNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(64, 32), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "try:\n",
    "    baseline_model = MulticlassNN(input_dim, num_classes).to(device)\n",
    "    baseline_model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    baseline_model.eval()\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Model file not found at {MODEL_PATH}. Run the baseline training script first.\")\n",
    "    #exit()\n",
    "\n",
    "class AttackNN_AllLogits(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 16), nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def create_attack_dataset_all_logits(model, train_tensor, test_tensor):\n",
    "    \"\"\"Extracts all logits (Z) as features for the attack model.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_outputs = model(train_tensor.to(device)).cpu()\n",
    "        test_outputs = model(test_tensor.to(device)).cpu()\n",
    "\n",
    "    attack_X = torch.cat([train_outputs, test_outputs], dim=0)\n",
    "    train_labels = torch.ones(len(train_outputs))\n",
    "    test_labels = torch.zeros(len(test_outputs))\n",
    "    attack_y = torch.cat([train_labels, test_labels], dim=0)\n",
    "    return attack_X, attack_y\n",
    "\n",
    "def run_mia_trial(attack_X, attack_y, random_seed):\n",
    "    \"\"\"Runs one trial of the MIA and returns the advantage score.\"\"\"\n",
    "    attack_X_train, attack_X_test, attack_y_train, attack_y_test = train_test_split(\n",
    "        attack_X, attack_y, test_size=0.3, random_state=random_seed, stratify=attack_y\n",
    "    )\n",
    "    attack_train_ds = TensorDataset(attack_X_train, attack_y_train)\n",
    "    attack_train_loader = DataLoader(attack_train_ds, batch_size=64, shuffle=True)\n",
    "    \n",
    "    n0 = np.sum(attack_y_train.numpy() == 0)\n",
    "    n1 = np.sum(attack_y_train.numpy() == 1)\n",
    "    pos_weight_val = n0 / n1\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight_val, dtype=torch.float32).to(device))\n",
    "\n",
    "    attack_model = AttackNN_AllLogits(attack_X.shape[1]).to(device)\n",
    "    optimizer = optim.Adam(attack_model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(50):\n",
    "        attack_model.train()\n",
    "        for xb, yb in attack_train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device).unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(attack_model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    attack_model.eval()\n",
    "    all_preds, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        attack_X_test_tensor = attack_X_test.to(device)\n",
    "        preds_logits = attack_model(attack_X_test_tensor).squeeze()\n",
    "        predicted_classes = (preds_logits > 0.0).float()\n",
    "        all_preds.extend(predicted_classes.cpu().numpy())\n",
    "        all_true.extend(attack_y_test.cpu().numpy())\n",
    "\n",
    "    report_dict = classification_report(all_true, all_preds, output_dict=True, zero_division=0)\n",
    "    TPR = report_dict['1.0']['recall']\n",
    "    FPR = 1 - report_dict['0.0']['recall']\n",
    "    advantage = TPR - FPR\n",
    "    return advantage\n",
    "\n",
    "print(\"Step 1: Creating attack dataset for the Baseline Model.\")\n",
    "attack_X_baseline, attack_y_baseline = create_attack_dataset_all_logits(baseline_model, X_train_tensor, X_test_tensor)\n",
    "\n",
    "num_trials = 10\n",
    "all_advantages = []\n",
    "\n",
    "print(f\"\\nStep 2: Running {num_trials} MIA trials on the Baseline Model...\")\n",
    "for i in range(num_trials):\n",
    "    seed = 42 + i\n",
    "    advantage = run_mia_trial(attack_X_baseline, attack_y_baseline, seed)\n",
    "    print(f\"  Trial {i+1}/{num_trials} (Seed: {seed}) -> MIA Advantage: {advantage:.4f}\")\n",
    "    all_advantages.append(advantage)\n",
    "\n",
    "mean_advantage = np.mean(all_advantages)\n",
    "std_advantage = np.std(all_advantages)\n",
    "\n",
    "print(\"\\n---\" * 10)\n",
    "print(\"Final Robust MIA Results for Baseline Model\")\n",
    "print(f\"  Mean MIA Advantage: {mean_advantage:.4f}\")\n",
    "print(f\"  Std Dev of MIA Advantage: {std_advantage:.4f}\")\n",
    "print(\"---\\n\" * 10)\n",
    "\n",
    "np.save(\"baseline_mia_copy.npy\", np.array([mean_advantage, std_advantage]))\n",
    "print(\"Successfully executed weighted MIA. Mean and Std Dev saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9783464f-9571-4c1b-a5e2-2d1c22eab13e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
