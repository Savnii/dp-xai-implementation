{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ab361-8343-4a93-bb15-a794fccef5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "FILE_SUFFIX = \"DP1\"\n",
    "\n",
    "print(\"Step 1: Loading the dataset.\")\n",
    "df = pd.read_csv('../datasets/drug_overdose.csv')\n",
    "\n",
    "print(\"Step 2: Cleaning the data and engineering features.\")\n",
    "\n",
    "df = df[df['Indicator'] == 'Number of Drug Overdose Deaths'].copy()\n",
    "\n",
    "df.dropna(subset=['Data Value'], inplace=True)\n",
    "\n",
    "cols_to_drop = ['State', 'Period', 'Footnote', 'Footnote Symbol',\n",
    "                'Predicted Value', 'Percent Complete', 'Percent Pending Investigation']\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "print(\"Step 3: Separating features (X) and target (y).\")\n",
    "y = df['Data Value']\n",
    "X = df[['State Name', 'Year']]\n",
    "\n",
    "print(\"Step 4: Identifying and preprocessing numerical and categorical columns.\")\n",
    "categorical_cols = ['State Name']\n",
    "numeric_cols = ['Year']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_processed = preprocessor.fit_transform(X).toarray()\n",
    "y_scaler = StandardScaler()\n",
    "y_processed = y_scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "joblib.dump(preprocessor.named_transformers_['num'], f\"artifacts/feature_scaler{FILE_SUFFIX}.joblib\")\n",
    "print(f\"Saved numeric feature scaler to artifacts/feature_scaler{FILE_SUFFIX}.joblib\")\n",
    "\n",
    "joblib.dump(preprocessor.named_transformers_['cat'], f\"artifacts/onehot_encoder{FILE_SUFFIX}.joblib\")\n",
    "print(f\"Saved one-hot encoder to artifacts/onehot_encoder{FILE_SUFFIX}.joblib\")\n",
    "\n",
    "joblib.dump(y_scaler, f\"artifacts/target_scaler{FILE_SUFFIX}.joblib\")\n",
    "print(f\"Saved target scaler to artifacts/target_scaler{FILE_SUFFIX}.joblib\")\n",
    "\n",
    "print(\"\\nStep 5: Combining and saving the cleaned data to a CSV file.\")\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "cleaned_df = pd.DataFrame(X_processed, columns=feature_names)\n",
    "cleaned_df['scaled_data_value'] = y_processed\n",
    "\n",
    "cleaned_df.to_csv(f'cleaned_overdose_data{FILE_SUFFIX}.csv', index=False)\n",
    "print(f\"File 'cleaned_overdose_data{FILE_SUFFIX}.csv' has been saved successfully.\")\n",
    "\n",
    "print(\"\\nData preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f7d04c-07d9-4310-910e-b3a571491125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score \n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from opacus import PrivacyEngine\n",
    "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
    "import shap\n",
    "\n",
    "FILE_SUFFIX = \"BS_ST\"\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "MICROBATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "NOISE_MULTIPLIER = 1.1\n",
    "MAX_GRAD_NORM = 1.0\n",
    "DELTA = 1e-4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"\\n--- STARTING TEST CASE 4: Baseline -> Student ---\")\n",
    "print(f\"FILE_SUFFIX = {FILE_SUFFIX}\\n\")\n",
    "\n",
    "print(\"\\nStep 1: Loading the cleaned dataset.\")\n",
    "df = pd.read_csv('cleaned_overdose_dataDP1.csv')\n",
    "\n",
    "print(\"Step 2: Separating features (X) and target (y).\")\n",
    "X = df.drop(columns='scaled_data_value').values.astype(np.float32)\n",
    "y = df['scaled_data_value'].values.astype(np.float32).reshape(-1, 1)\n",
    "feature_names = df.drop(columns='scaled_data_value').columns.tolist()\n",
    "INPUT_DIM = X.shape[1]\n",
    "\n",
    "print(\"Step 3: Splitting the data into training and testing sets.\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_ds = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "test_ds = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=64)\n",
    "\n",
    "print(\"Step 4: Building the feedforward neural network model.\")\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "teacher_model = RegressionNN(INPUT_DIM).to(device)\n",
    "\n",
    "print(\"Step 5: Training the model with DP-SGD.\")\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(teacher_model.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "privacy_engine = PrivacyEngine(secure_mode=False)\n",
    "\n",
    "teacher_model, optimizer, train_loader = privacy_engine.make_private(\n",
    "    module=teacher_model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_loader,\n",
    "    noise_multiplier=NOISE_MULTIPLIER,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    ")\n",
    "\n",
    "print(f\"Training with Noise Multiplier: {NOISE_MULTIPLIER} and Delta: {DELTA}\")\n",
    "final_epsilon = 0.0 # To store the final epsilon\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    teacher_model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    with BatchMemoryManager(\n",
    "        data_loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        max_physical_batch_size=MICROBATCH_SIZE\n",
    "    ) as train_loader_managed:\n",
    "        for xb, yb in train_loader_managed:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = teacher_model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader.dataset)\n",
    "    epsilon = privacy_engine.get_epsilon(DELTA)\n",
    "    final_epsilon = epsilon # Update epsilon each epoch\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}] - Train MSE: {avg_loss:.4f}  (ε = {epsilon:.2f})\")\n",
    "\n",
    "print(\"\\nStep 6: Evaluating the model on the test data.\")\n",
    "teacher_model.eval()\n",
    "all_preds, all_true = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        preds = teacher_model(xb).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_true.extend(yb.numpy())\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"DP-SGD TEACHER MODEL EVALUATION METRICS\")\n",
    "print(\"=\"*40)\n",
    "all_true_np = np.array(all_true)\n",
    "all_preds_np = np.array(all_preds)\n",
    "test_mse = mean_squared_error(all_true_np, all_preds_np)\n",
    "test_mae = mean_absolute_error(all_true_np, all_preds_np)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(all_true_np, all_preds_np)\n",
    "\n",
    "print(\"--- Utility Metrics ---\")\n",
    "print(f\"Mean Squared Error (MSE):      {test_mse:.4f}\")\n",
    "print(f\"**Mean Absolute Error (MAE):** {test_mae:.4f}\")\n",
    "print(f\"**Root Mean Squared Error (RMSE):** {test_rmse:.4f}\")\n",
    "print(f\"**R-squared (R²):** {test_r2:.4f}\")\n",
    "print(\"\\n--- Privacy Metrics ---\")\n",
    "print(f\"**Noise Multiplier:** {NOISE_MULTIPLIER}\")\n",
    "print(f\"**Epsilon (Final):** {final_epsilon:.4f}\")\n",
    "print(f\"**Delta:** {DELTA}\")\n",
    "print(\"=\"*40 + \"\\n\")\n",
    "\n",
    "print(\"\\nStep 7: Generating SHAP explanations using DeepExplainer.\")\n",
    "dp_model_unwrapped = teacher_model._module\n",
    "clean_model = RegressionNN(INPUT_DIM).to(device)\n",
    "clean_model.load_state_dict(dp_model_unwrapped.state_dict())\n",
    "clean_model.eval()\n",
    "\n",
    "background_size = min(100, len(X_train))\n",
    "background_data = torch.tensor(\n",
    "    X_train[np.random.choice(X_train.shape[0], background_size, replace=False)],\n",
    "    dtype=torch.float32\n",
    ").to(device)\n",
    "X_explain = torch.tensor(X_test[:20], dtype=torch.float32).to(device)\n",
    "\n",
    "explainer = shap.DeepExplainer(clean_model, background_data)\n",
    "shap_values = explainer.shap_values(X_explain)\n",
    "\n",
    "if shap_values.ndim > 1:\n",
    "    shap_values = np.squeeze(shap_values, axis=-1)\n",
    "\n",
    "mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
    "feature_importance = dict(zip(feature_names, mean_abs_shap))\n",
    "\n",
    "print(\"\\nAverage absolute SHAP values (feature importance):\")\n",
    "sorted_importance = sorted(feature_importance.items(), key=lambda item: item[1], reverse=True)\n",
    "for feature, value in sorted_importance[:10]:\n",
    "    print(f\"{feature}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nStep 8: Saving model and SHAP results.\")\n",
    "TEACHER_SAVE_PATH = f\"artifacts/dpsgd_regression_model{FILE_SUFFIX}.pt\"\n",
    "torch.save(dp_model_unwrapped.state_dict(), TEACHER_SAVE_PATH)\n",
    "print(f\"DP-SGD Teacher model saved to {TEACHER_SAVE_PATH}\")\n",
    "\n",
    "shap_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'mean_abs_shap': mean_abs_shap\n",
    "}).sort_values('mean_abs_shap', ascending=False)\n",
    "SHAP_SAVE_PATH = f'teacher_shap_importance_reg{FILE_SUFFIX}.csv' \n",
    "shap_df.to_csv(SHAP_SAVE_PATH, index=False)\n",
    "print(f\"SHAP feature importance saved to '{SHAP_SAVE_PATH}'.\")\n",
    "\n",
    "print(\"\\n--- Process Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202fab1-18f9-4355-b844-7a2eefcc9445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "FILE_SUFFIX = \"BS_ST\"\n",
    "\n",
    "print(\"Step 1: Loading cleaned data...\")\n",
    "df = pd.read_csv('cleaned_overdose_dataDP1.csv')\n",
    "\n",
    "print(\"Step 2: Splitting features and target...\")\n",
    "X = df.drop(columns=['scaled_data_value']).values.astype(np.float32)\n",
    "y = df['scaled_data_value'].values.astype(np.float32)\n",
    "\n",
    "print(\"Step 3: Creating train-test split...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "print(\"Step 4: Loading trained DP-SGD model...\")\n",
    "\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = RegressionNN(input_dim).to(device)\n",
    "\n",
    "weights_path = f\"artifacts/dpsgd_regression_model{FILE_SUFFIX}.pt\" \n",
    "if not os.path.exists(weights_path):\n",
    "    raise FileNotFoundError(f\"Trained model weights not found at {weights_path}.\")\n",
    "\n",
    "loaded_state_dict = torch.load(weights_path, map_location=device)\n",
    "model.load_state_dict(loaded_state_dict)\n",
    "model.eval()\n",
    "\n",
    "def model_inversion_attack(model, target_value, input_dim, num_steps=1000, lr=0.01, lambda_reg=1e-4):\n",
    "    \"\"\"\n",
    "    Attempt to reconstruct a plausible input vector that produces the given target_value.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    recon_input = torch.randn((1, input_dim), requires_grad=True, device=device)\n",
    "    optimizer = optim.Adam([recon_input], lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    target_tensor = torch.tensor([[target_value]], dtype=torch.float32, device=device)\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(recon_input)\n",
    "        loss = loss_fn(prediction, target_tensor)\n",
    "        loss += lambda_reg * torch.norm(recon_input)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}/{num_steps} - Pred: {prediction.item():.4f} - Loss: {loss.item():.6f}\")\n",
    "\n",
    "    return recon_input.detach().cpu().numpy()\n",
    "\n",
    "print(\"\\nStep 5: Running Model Inversion Attack on DP-SGD model...\")\n",
    "\n",
    "known_targets = y_test[:5]\n",
    "\n",
    "reconstructed_inputs = []\n",
    "for tval in known_targets:\n",
    "    print(f\"\\nReconstructing input for target value: {tval:.4f}\")\n",
    "    recon = model_inversion_attack(model, tval, input_dim)\n",
    "    reconstructed_inputs.append(recon)\n",
    "\n",
    "reconstructed_inputs = np.vstack(reconstructed_inputs)\n",
    "\n",
    "print(\"\\nReconstructed Feature Vectors (in scaled feature space):\")\n",
    "print(reconstructed_inputs)\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "np.save(f\"artifacts/reconstructed_inputs_dpsgd{FILE_SUFFIX}.npy\", reconstructed_inputs)\n",
    "print(f\"\\nSaved reconstructed inputs to artifacts/reconstructed_inputs_dpsgd{FILE_SUFFIX}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f813dc1-ba7d-488a-8500-af30b3e47a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "FILE_SUFFIX = \"BS_ST\"\n",
    "\n",
    "print(\"Step 1: Loading cleaned data...\")\n",
    "df = pd.read_csv('cleaned_overdose_dataDP1.csv')\n",
    "if not os.path.exists('cleaned_overdose_dataDP1.csv'):\n",
    "    raise FileNotFoundError(f\"Dataset not found\")\n",
    "\n",
    "target_col = \"scaled_data_value\"\n",
    "if target_col not in df.columns:\n",
    "    raise KeyError(f\"Target column '{target_col}' not found in dataset. Columns are: {df.columns.tolist()}\")\n",
    "\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "print(\"Loading saved scalers/encoders and reconstructed inputs...\")\n",
    "\n",
    "feature_scaler_path = f\"artifacts/feature_scalerDP1.joblib\"\n",
    "onehot_encoder_path = f\"artifacts/onehot_encoderDP1.joblib\"\n",
    "recon_path = f\"artifacts/reconstructed_inputs_dpsgd{FILE_SUFFIX}.npy\"\n",
    "\n",
    "for p in [feature_scaler_path, onehot_encoder_path, recon_path]:\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Required file not found at {p}\")\n",
    "\n",
    "feature_scaler = joblib.load(feature_scaler_path)\n",
    "onehot_encoder = joblib.load(onehot_encoder_path)\n",
    "recon_scaled = np.load(recon_path)\n",
    "\n",
    "num_features = 1\n",
    "cat_features = recon_scaled.shape[1] - num_features\n",
    "\n",
    "recon_num = recon_scaled[:, :num_features]\n",
    "recon_cat = recon_scaled[:, num_features:]\n",
    "\n",
    "print(\"Reversing scaling to get real feature values...\")\n",
    "\n",
    "recon_years = feature_scaler.inverse_transform(recon_num).flatten()\n",
    "\n",
    "recon_states = onehot_encoder.inverse_transform(recon_cat).flatten()\n",
    "\n",
    "recon_df = pd.DataFrame({\n",
    "    \"Year\": np.round(recon_years).astype(int),\n",
    "    \"State Name\": recon_states\n",
    "})\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "save_path = f\"artifacts/reconstructed_real_values_dpsgd{FILE_SUFFIX}.csv\"\n",
    "recon_df.to_csv(save_path, index=False)\n",
    "print(f\"Reconstructed feature values saved to {save_path}\")\n",
    "\n",
    "print(\"Showing reconstructed sensitive features (State Name, Year)...\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(range(len(recon_df)), recon_df[\"Year\"], c=\"red\", label=\"Reconstructed Year\", alpha=0.7)\n",
    "plt.title(\"Recovered Years from DP-SGD Model Inversion Attack\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Year\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSample of reverse-engineered sensitive data:\")\n",
    "print(recon_df.head(10))\n",
    "\n",
    "print(\"\\n Done — reconstructed original 'State Name' and 'Year' from DP-SGD model outputs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93594f06-a90f-47a9-bfd9-9944d18ec000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "FILE_SUFFIX = \"BS_ST\" \n",
    "\n",
    "BASELINE_SUFFIX = \"1\" \n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "print(f\"--- STARTING TEST CASE 4 (User) / TC9 (PDF): Baseline -> Student ---\")\n",
    "print(f\"FILE_SUFFIX = {FILE_SUFFIX}\\n\")\n",
    "\n",
    "print(\"Step 1: Loading BASELINE preprocessed data and splitting.\")\n",
    "\n",
    "df = pd.read_csv(f'cleaned_overdose_data{BASELINE_SUFFIX}.csv')\n",
    "X = df.drop(columns='scaled_data_value').values.astype(np.float32)\n",
    "y = df['scaled_data_value'].values.astype(np.float32).reshape(-1, 1)\n",
    "input_dim = X.shape[1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train_tensor = torch.tensor(X_train).to(device)\n",
    "\n",
    "print(f\"Step 2: Loading trained BASELINE Teacher Model (Input Dim: {input_dim}).\")\n",
    "teacher_model = RegressionNN(input_dim).to(device)\n",
    "\n",
    "weights_path = f\"artifacts/regression_model{BASELINE_SUFFIX}.pt\" \n",
    "loaded_state_dict = torch.load(weights_path, map_location=device)\n",
    "teacher_model.load_state_dict(loaded_state_dict)\n",
    "teacher_model.eval()\n",
    "\n",
    "print(\"Step 3: Generating 'perfect' Soft Targets.\")\n",
    "with torch.no_grad():\n",
    "    Y_soft_tensor = teacher_model(X_train_tensor)\n",
    "Y_soft = Y_soft_tensor.cpu().numpy()\n",
    "\n",
    "np.savez(f'kd_targets{FILE_SUFFIX}.npz', \n",
    "         X_train=X_train, \n",
    "         Y_train=y_train, \n",
    "         Y_soft=Y_soft, \n",
    "         X_test=X_test, \n",
    "         Y_test=y_test,\n",
    "         input_dim=np.array([input_dim])\n",
    ")\n",
    "print(f\"\\nSoft targets generated and saved to kd_targets{FILE_SUFFIX}.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f09664-b1f6-4173-8b7e-7042f4160e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score \n",
    "import os\n",
    "\n",
    "FILE_SUFFIX = \"BS_ST\" \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ALPHA = 0.9\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "\n",
    "print(\"Step 1: Loading data and 'perfect' targets.\")\n",
    "\n",
    "kd_data = np.load(f'kd_targets{FILE_SUFFIX}.npz') \n",
    "X_train = kd_data['X_train']\n",
    "Y_train = kd_data['Y_train']\n",
    "Y_soft = kd_data['Y_soft']\n",
    "X_test = kd_data['X_test']\n",
    "Y_test = kd_data['Y_test']\n",
    "INPUT_DIM = kd_data['input_dim'][0]\n",
    "\n",
    "class StudentNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "student_model = StudentNN(INPUT_DIM).to(device)\n",
    "train_ds = TensorDataset(\n",
    "    torch.tensor(X_train), \n",
    "    torch.tensor(Y_train), \n",
    "    torch.tensor(Y_soft)\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_ds = TensorDataset(torch.tensor(X_test), torch.tensor(Y_test))\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "def kd_loss(Y_pred, Y_hard, Y_soft, alpha=ALPHA):\n",
    "    soft_loss = nn.MSELoss()(Y_pred, Y_soft)\n",
    "    hard_loss = nn.MSELoss()(Y_pred, Y_hard)\n",
    "    return alpha * soft_loss + (1.0 - alpha) * hard_loss\n",
    "\n",
    "print(f\"Step 4: Training Student Model (epochs={EPOCHS}, alpha={ALPHA})...\")\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    student_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for xb, Y_hard_batch, Y_soft_batch in train_loader:\n",
    "        xb, Y_hard_batch, Y_soft_batch = (\n",
    "            xb.to(device), \n",
    "            Y_hard_batch.to(device), \n",
    "            Y_soft_batch.to(device)\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        Y_pred = student_model(xb)\n",
    "        loss = kd_loss(Y_pred, Y_hard_batch, Y_soft_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * xb.size(0)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}] - Train Loss: {epoch_loss/len(X_train):.4f}\")\n",
    "\n",
    "student_model.eval()\n",
    "all_preds, all_true = [], []\n",
    "with torch.no_grad():\n",
    "    for xb_test, Y_test_batch in test_loader:\n",
    "        preds = student_model(xb_test.to(device)).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_true.extend(Y_test_batch.numpy())\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"KD STUDENT (from BASELINE) METRICS (Suffix: {FILE_SUFFIX})\")\n",
    "print(\"=\"*40)\n",
    "all_true_np = np.array(all_true)\n",
    "all_preds_np = np.array(all_preds)\n",
    "test_mse = mean_squared_error(all_true_np, all_preds_np)\n",
    "test_mae = mean_absolute_error(all_true_np, all_preds_np)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(all_true_np, all_preds_np)\n",
    "\n",
    "print(\"--- Utility Metrics ---\")\n",
    "print(f\"Mean Squared Error (MSE):      {test_mse:.4f}\")\n",
    "print(f\"**Mean Absolute Error (MAE):** {test_mae:.4f}\")\n",
    "print(f\"**Root Mean Squared Error (RMSE):** {test_rmse:.4f}\")\n",
    "print(f\"**R-squared (R²):** {test_r2:.4f}\")\n",
    "print(\"\\n--- Privacy Metrics ---\")\n",
    "print(\"**Epsilon:** Not Applicable (N/A) - Teacher was non-private.\")\n",
    "print(\"**Delta:** Not Applicable (N/A)\")\n",
    "print(\"=\"*40 + \"\\n\")\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "STUDENT_SAVE_PATH = f\"artifacts/kd_student_model{FILE_SUFFIX}.pt\" \n",
    "torch.save(student_model.state_dict(), STUDENT_SAVE_PATH)\n",
    "print(f\"\\n KD Student model (from Baseline) trained and saved to {STUDENT_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f6c0e-f0d6-42b2-8c4b-9579aca01273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "FILE_SUFFIX = \"BS_ST\" \n",
    "\n",
    "BASELINE_SUFFIX = \"1\" \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "print(\"Step 1: Loading data and trained Student Model (from Baseline).\")\n",
    "\n",
    "kd_data = np.load(f'kd_targets{FILE_SUFFIX}.npz')\n",
    "X_test = kd_data['X_test']\n",
    "Y_test = kd_data['Y_test']\n",
    "X_train = kd_data['X_train']\n",
    "INPUT_DIM = kd_data['input_dim'][0]\n",
    "\n",
    "feature_names = pd.read_csv(f'cleaned_overdose_data{BASELINE_SUFFIX}.csv').drop(columns='scaled_data_value').columns.tolist()\n",
    "\n",
    "class StudentNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "student_model = StudentNN(INPUT_DIM).to(device)\n",
    "\n",
    "student_model.load_state_dict(torch.load(f\"artifacts/kd_student_model{FILE_SUFFIX}.pt\", map_location=device))\n",
    "student_model.eval()\n",
    "\n",
    "print(\"\\nStep 2: Checking Explainability (SHAP).\")\n",
    "background_size = min(100, len(X_train))\n",
    "background_data = torch.tensor(\n",
    "    X_train[np.random.choice(X_train.shape[0], background_size, replace=False)],\n",
    "    dtype=torch.float32\n",
    ").to(device)\n",
    "X_explain = torch.tensor(X_test[:20], dtype=torch.float32).to(device)\n",
    "\n",
    "explainer = shap.DeepExplainer(student_model, background_data)\n",
    "shap_values = explainer.shap_values(X_explain)\n",
    "\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[0]\n",
    "if shap_values.ndim > 2:\n",
    "    shap_values = np.squeeze(shap_values)\n",
    "\n",
    "mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
    "\n",
    "print(\"**Average absolute SHAP values (feature importance) for Baseline->Student Model:**\")\n",
    "student_shap_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'mean_abs_shap': mean_abs_shap\n",
    "}).sort_values('mean_abs_shap', ascending=False)\n",
    "\n",
    "teacher_shap_df = pd.read_csv(f'shap_feature_importance{BASELINE_SUFFIX}.csv') \n",
    "print(f\"\\nTop 5 features (Baseline->Student):\")\n",
    "print(student_shap_df.head())\n",
    "print(f\"\\nTop 5 features (Baseline Teacher):\")\n",
    "print(teacher_shap_df.head())\n",
    "\n",
    "student_shap_df.to_csv(f'student_shap_importance_reg{FILE_SUFFIX}.csv', index=False)\n",
    "print(f\"\\nSaved Student SHAP feature importance to 'student_shap_importance_reg{FILE_SUFFIX}.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac0982-0111-4bd8-8e68-3dc915aed151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "FILE_SUFFIX = \"BS_ST\" \n",
    "\n",
    "BASELINE_SUFFIX = \"1\" \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Step 1: Loading data, model, and BASELINE scalers...\")\n",
    "\n",
    "kd_data = np.load(f'kd_targets{FILE_SUFFIX}.npz')\n",
    "Y_test = kd_data['Y_test']\n",
    "INPUT_DIM = kd_data['input_dim'][0]\n",
    "\n",
    "class StudentNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "student_model = StudentNN(INPUT_DIM).to(device)\n",
    "\n",
    "weights_path = f\"artifacts/kd_student_model{FILE_SUFFIX}.pt\"\n",
    "student_model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "student_model.eval()\n",
    "\n",
    "feature_scaler_path = f\"artifacts/feature_scaler{BASELINE_SUFFIX}.joblib\"\n",
    "onehot_encoder_path = f\"artifacts/onehot_encoder{BASELINE_SUFFIX}.joblib\"\n",
    "feature_scaler = joblib.load(feature_scaler_path)\n",
    "onehot_encoder = joblib.load(onehot_encoder_path)\n",
    "\n",
    "def model_inversion_attack(model, target_value, input_dim, num_steps=1000, lr=0.01, lambda_reg=1e-4):\n",
    "\n",
    "    model.eval()\n",
    "    recon_input = torch.randn((1, input_dim), requires_grad=True, device=device)\n",
    "    optimizer = optim.Adam([recon_input], lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    target_tensor = torch.tensor([[target_value]], dtype=torch.float32, device=device)\n",
    "    for step in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(recon_input)\n",
    "        loss = loss_fn(prediction, target_tensor)\n",
    "        loss += lambda_reg * torch.norm(recon_input)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}/{num_steps} - Pred: {prediction.item():.4f} - Loss: {loss.item():.6f}\")\n",
    "    return recon_input.detach().cpu().numpy()\n",
    "\n",
    "print(\"\\nStep 2: Running Model Inversion Attack on Baseline->Student model...\")\n",
    "known_targets = Y_test[:5].flatten()\n",
    "\n",
    "reconstructed_inputs = []\n",
    "for tval in known_targets:\n",
    "    print(f\"\\nReconstructing input for target value: {tval:.4f}\")\n",
    "    recon = model_inversion_attack(student_model, tval, INPUT_DIM)\n",
    "    reconstructed_inputs.append(recon)\n",
    "\n",
    "reconstructed_inputs = np.vstack(reconstructed_inputs)\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "np.save(f\"artifacts/reconstructed_inputs_student{FILE_SUFFIX}.npy\", reconstructed_inputs)\n",
    "print(f\"\\nSaved reconstructed inputs to artifacts/reconstructed_inputs_student{FILE_SUFFIX}.npy\")\n",
    "\n",
    "print(\"\\nStep 3: Reversing scaling to get real feature values...\")\n",
    "\n",
    "recon_scaled = reconstructed_inputs\n",
    "num_features = 1\n",
    "cat_features = recon_scaled.shape[1] - num_features\n",
    "recon_num = recon_scaled[:, :num_features]\n",
    "recon_cat = recon_scaled[:, num_features:]\n",
    "recon_years = feature_scaler.inverse_transform(recon_num).flatten()\n",
    "recon_states = onehot_encoder.inverse_transform(recon_cat).flatten()\n",
    "recon_df = pd.DataFrame({\n",
    "    \"Year\": np.round(recon_years).astype(int),\n",
    "    \"State Name\": recon_states\n",
    "})\n",
    "\n",
    "save_path = f\"artifacts/reconstructed_real_values_student{FILE_SUFFIX}.csv\"\n",
    "recon_df.to_csv(save_path, index=False)\n",
    "print(f\"Reconstructed feature values saved to {save_path}\")\n",
    "\n",
    "print(\"\\nStep 4: Showing reconstructed sensitive features (State Name, Year)...\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(range(len(recon_df)), recon_df[\"Year\"], c=\"green\", label=\"Reconstructed Year (Baseline->Student)\", alpha=0.7) # <-- Changed color for clarity\n",
    "plt.title(\"Recovered Years from Baseline->Student Model Inversion Attack\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Year\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSample of reverse-engineered sensitive data (from Baseline->Student Model):\")\n",
    "print(recon_df.head(10))\n",
    "print(\"\\n Done — reconstructed original 'State Name' and 'Year' from Baseline->Student model.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
