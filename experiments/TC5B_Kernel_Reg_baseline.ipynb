{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b395bcf9-a856-4bc1-b7c9-bee5f21d0c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "FILE_SUFFIX = \"1\"\n",
    "\n",
    "print(\"Step 1: Loading the dataset.\")\n",
    "df = pd.read_csv('../datasets/drug_overdose.csv')\n",
    "\n",
    "print(\"Step 2: Cleaning the data and engineering features.\")\n",
    "\n",
    "df = df[df['Indicator'] == 'Number of Drug Overdose Deaths'].copy()\n",
    "\n",
    "df.dropna(subset=['Data Value'], inplace=True)\n",
    "\n",
    "cols_to_drop = ['State', 'Period', 'Footnote', 'Footnote Symbol',\n",
    "                'Predicted Value', 'Percent Complete', 'Percent Pending Investigation']\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "print(\"Step 3: Separating features (X) and target (y).\")\n",
    "y = df['Data Value']\n",
    "X = df[['State Name', 'Year']]\n",
    "\n",
    "print(\"Step 4: Identifying and preprocessing numerical and categorical columns.\")\n",
    "categorical_cols = ['State Name']\n",
    "numeric_cols = ['Year']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_processed = preprocessor.fit_transform(X).toarray()\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_processed = y_scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "joblib.dump(preprocessor.named_transformers_['num'], f\"artifacts/feature_scaler{FILE_SUFFIX}.joblib\")\n",
    "print(f\"Saved numeric feature scaler to artifacts/feature_scaler{FILE_SUFFIX}.joblib\")\n",
    "\n",
    "joblib.dump(preprocessor.named_transformers_['cat'], f\"artifacts/onehot_encoder{FILE_SUFFIX}.joblib\")\n",
    "print(f\"Saved one-hot encoder to artifacts/onehot_encoder{FILE_SUFFIX}.joblib\")\n",
    "\n",
    "joblib.dump(y_scaler, f\"artifacts/target_scaler{FILE_SUFFIX}.joblib\")\n",
    "print(f\"Saved target scaler to artifacts/target_scaler{FILE_SUFFIX}.joblib\")\n",
    "\n",
    "print(\"\\nStep 5: Combining and saving the cleaned data to a CSV file.\")\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "cleaned_df = pd.DataFrame(X_processed, columns=feature_names)\n",
    "cleaned_df['scaled_data_value'] = y_processed\n",
    "\n",
    "cleaned_df.to_csv(f'cleaned_overdose_data{FILE_SUFFIX}.csv', index=False)\n",
    "print(f\"File 'cleaned_overdose_data{FILE_SUFFIX}.csv' has been saved successfully.\")\n",
    "\n",
    "print(\"\\nData preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098e5943-1dfa-45a9-a8a9-78f5ae2829c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import shap\n",
    "\n",
    "print(\"\\nStep 1: Loading the cleaned dataset.\")\n",
    "df = pd.read_csv(f'cleaned_overdose_data{FILE_SUFFIX}.csv')\n",
    "\n",
    "print(\"Step 2: Separating features (X) and target (y).\")\n",
    "\n",
    "X = df.drop(columns='scaled_data_value').values.astype(np.float32)\n",
    "y = df['scaled_data_value'].values.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "feature_names = df.drop(columns='scaled_data_value').columns.tolist()\n",
    "\n",
    "print(\"Step 3: Splitting the data into training and testing sets.\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_ds = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "test_ds = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=64)\n",
    "\n",
    "print(\"Step 4: Building the feedforward neural network model.\")\n",
    "\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Step 5: Training the model.\")\n",
    "\n",
    "baseline_model = RegressionNN(X_train.shape[1]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(baseline_model.parameters(), lr=0.001)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    baseline_model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = baseline_model(xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Training MSE: {avg_loss:.4f}\")\n",
    "\n",
    "MODEL_SAVE_PATH = f\"regression_baseline_model{FILE_SUFFIX}.pth\" \n",
    "torch.save(baseline_model.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"\\nBaseline model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "print(\"\\nStep 6: Evaluating the model on the test data.\")\n",
    "\n",
    "baseline_model.eval()\n",
    "all_preds, all_true = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        preds = baseline_model(xb).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_true.extend(yb.numpy())\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"MODEL EVALUATION METRICS (Baseline)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "all_true_np = np.array(all_true)\n",
    "all_preds_np = np.array(all_preds)\n",
    "\n",
    "test_mse = mean_squared_error(all_true_np, all_preds_np)\n",
    "test_mae = mean_absolute_error(all_true_np, all_preds_np)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_r2 = r2_score(all_true_np, all_preds_np)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE):      {test_mse:.4f}\")\n",
    "print(f\"**Mean Absolute Error (MAE):** {test_mae:.4f}\")\n",
    "print(f\"**Root Mean Squared Error (RMSE):** {test_rmse:.4f}\")\n",
    "print(f\"**R-squared (R²):** {test_r2:.4f}\")\n",
    "\n",
    "print(\"\\n--- Privacy Metrics ---\")\n",
    "print(\"**Epsilon:** Not Applicable (N/A) - This is a non-private baseline model.\")\n",
    "print(\"**Delta:** Not Applicable (N/A) - This is a non-private baseline model.\")\n",
    "print(\"=\"*40 + \"\\n\")\n",
    "\n",
    "print(\"\\nStep 7: Generating SHAP explanations using DeepExplainer.\")\n",
    "\n",
    "background_size = min(100, len(X_train))\n",
    "background_data = torch.tensor(\n",
    "    X_train[np.random.choice(X_train.shape[0], background_size, replace=False)],\n",
    "    dtype=torch.float32\n",
    ").to(device)\n",
    "\n",
    "X_explain = torch.tensor(X_test[:20], dtype=torch.float32).to(device)\n",
    "\n",
    "explainer = shap.DeepExplainer(baseline_model, background_data)\n",
    "shap_values = explainer.shap_values(X_explain)\n",
    "\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[0]\n",
    "\n",
    "if shap_values.ndim > 2:\n",
    "    shap_values = np.squeeze(shap_values)\n",
    "\n",
    "mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
    "feature_importance = dict(zip(feature_names, mean_abs_shap))\n",
    "\n",
    "print(\"\\n**Average absolute SHAP values (feature importance):**\")\n",
    "for f, val in feature_importance.items():\n",
    "    print(f\"{f}: {float(val):.4f}\")\n",
    "\n",
    "print(\"\\nStep 8: Saving SHAP feature importance to CSV.\")\n",
    "\n",
    "shap_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'mean_abs_shap': mean_abs_shap\n",
    "}).sort_values('mean_abs_shap', ascending=False)\n",
    "\n",
    "shap_df.to_csv(f'shap_feature_importance{FILE_SUFFIX}.csv', index=False) \n",
    "print(f\"Saved SHAP feature importance to 'shap_feature_importance{FILE_SUFFIX}.csv'.\")\n",
    "\n",
    "print(\"\\nModel training, evaluation, and SHAP explainability are complete.\")\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "torch.save(baseline_model.state_dict(), f\"artifacts/regression_model{FILE_SUFFIX}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690e0e65-5fa9-4220-a35f-2a2baf710030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import os\n",
    "\n",
    "print(f\"\\n--- STARTING TEST CASE 5: KernelExplainer on Baseline ---\")\n",
    "FILE_SUFFIX = \"1\" \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "print(\"Loading baseline data...\")\n",
    "df = pd.read_csv(f'cleaned_overdose_data{FILE_SUFFIX}.csv')\n",
    "X = df.drop(columns='scaled_data_value').values.astype(np.float32)\n",
    "feature_names = df.drop(columns='scaled_data_value').columns.tolist()\n",
    "INPUT_DIM = X.shape[1]\n",
    "\n",
    "print(\"Loading baseline model...\")\n",
    "model = RegressionNN(INPUT_DIM).to(device)\n",
    "weights_path = f\"artifacts/regression_model{FILE_SUFFIX}.pt\"\n",
    "model.load_state_dict(torch.load(weights_path, map_location=device, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "def predict_wrapper(x_numpy):\n",
    "    x_tensor = torch.tensor(x_numpy).to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = model(x_tensor).cpu().numpy()\n",
    "    return preds\n",
    "\n",
    "print(\"Creating K-Means background data for SHAP...\")\n",
    "\n",
    "X_summary = shap.kmeans(X, 50) \n",
    "print(\"Running shap.KernelExplainer...\")\n",
    "\n",
    "explainer = shap.KernelExplainer(predict_wrapper, X_summary)\n",
    "\n",
    "shap_values = explainer.shap_values(X[:50])\n",
    "\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[0]\n",
    "if shap_values.ndim > 2:\n",
    "    shap_values = np.squeeze(shap_values)\n",
    "\n",
    "mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
    "\n",
    "print(\"\\n**Average absolute SHAP values (KernelExplainer - Baseline):**\")\n",
    "shap_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'mean_abs_shap': mean_abs_shap\n",
    "}).sort_values('mean_abs_shap', ascending=False)\n",
    "\n",
    "print(shap_df.head())\n",
    "\n",
    "SAVE_PATH = f'shap_feature_importance_KERNEL_{FILE_SUFFIX}.csv'\n",
    "shap_df.to_csv(SAVE_PATH, index=False)\n",
    "print(f\"\\nSaved KernelExplainer SHAP feature importance to '{SAVE_PATH}'.\")\n",
    "print(\"\\n--- Baseline Model (KernelExplainer) Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cbe941-63d7-4a84-9bda-528dd5c10f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "print(\"Step 1: Loading cleaned data...\")\n",
    "data_path = f\"cleaned_overdose_data{FILE_SUFFIX}.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(\"Step 2: Splitting features and target...\")\n",
    "X = df.drop(columns=['scaled_data_value']).values.astype(np.float32)\n",
    "y = df['scaled_data_value'].values.astype(np.float32)\n",
    "\n",
    "print(\"Step 3: Creating train-test split...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "print(\"Step 4: Loading trained model...\")\n",
    "\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "baseline_model = RegressionNN(input_dim).to(device)\n",
    "\n",
    "weights_path = f\"artifacts/regression_model{FILE_SUFFIX}.pt\"\n",
    "if not os.path.exists(weights_path):\n",
    "    raise FileNotFoundError(f\"Trained model weights not found at {weights_path}\")\n",
    "\n",
    "baseline_model.load_state_dict(torch.load(weights_path, map_location=device, weights_only=True))\n",
    "baseline_model.eval()\n",
    "\n",
    "def model_inversion_attack(model, target_value, input_dim, num_steps=1000, lr=0.01, lambda_reg=1e-4):\n",
    "    \"\"\"\n",
    "    Attempt to reconstruct a plausible input vector that produces the given target_value.\n",
    "    \"\"\"\n",
    "    baseline_model.eval()\n",
    "\n",
    "    recon_input = torch.randn((1, input_dim), requires_grad=True, device=device)\n",
    "\n",
    "    optimizer = optim.Adam([recon_input], lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    target_tensor = torch.tensor([[target_value]], dtype=torch.float32, device=device)\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(recon_input)\n",
    "        loss = loss_fn(prediction, target_tensor)\n",
    "        loss += lambda_reg * torch.norm(recon_input)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}/{num_steps} - Pred: {prediction.item():.4f} - Loss: {loss.item():.6f}\")\n",
    "\n",
    "    return recon_input.detach().cpu().numpy()\n",
    "\n",
    "print(\"\\nStep 5: Running Model Inversion Attack...\")\n",
    "\n",
    "known_targets = y_test[:5]\n",
    "\n",
    "reconstructed_inputs = []\n",
    "for tval in known_targets:\n",
    "    print(f\"\\nReconstructing input for target value: {tval:.4f}\")\n",
    "    recon = model_inversion_attack(baseline_model, tval, input_dim)\n",
    "    reconstructed_inputs.append(recon)\n",
    "\n",
    "reconstructed_inputs = np.vstack(reconstructed_inputs)\n",
    "\n",
    "print(\"\\nReconstructed Feature Vectors (in scaled feature space):\")\n",
    "print(reconstructed_inputs)\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "np.save(f\"artifacts/reconstructed_inputs{FILE_SUFFIX}.npy\", reconstructed_inputs) \n",
    "print(f\"\\nSaved reconstructed inputs to artifacts/reconstructed_inputs{FILE_SUFFIX}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f5579-9965-4eb5-a751-63b6d583e05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "print(\"Loading original cleaned dataset...\")\n",
    "data_path = f\"cleaned_overdose_data{FILE_SUFFIX}.csv\"\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"Dataset not found at {data_path}\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "target_col = \"scaled_data_value\"\n",
    "if target_col not in df.columns:\n",
    "    raise KeyError(f\"Target column '{target_col}' not found in dataset. Columns are: {df.columns.tolist()}\")\n",
    "\n",
    "X = df.drop(columns=[target_col])\n",
    "\n",
    "print(\"Loading saved scalers/encoders and reconstructed inputs...\")\n",
    "\n",
    "feature_scaler_path = f\"artifacts/feature_scaler{FILE_SUFFIX}.joblib\"\n",
    "onehot_encoder_path = f\"artifacts/onehot_encoder{FILE_SUFFIX}.joblib\"\n",
    "recon_path = f\"artifacts/reconstructed_inputs{FILE_SUFFIX}.npy\"\n",
    "\n",
    "for p in [feature_scaler_path, onehot_encoder_path, recon_path]:\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Required file not found at {p}\")\n",
    "\n",
    "feature_scaler = joblib.load(feature_scaler_path)\n",
    "onehot_encoder = joblib.load(onehot_encoder_path)\n",
    "recon_scaled = np.load(recon_path)\n",
    "\n",
    "num_features = 1\n",
    "cat_features = recon_scaled.shape[1] - num_features\n",
    "\n",
    "recon_num = recon_scaled[:, :num_features]\n",
    "recon_cat = recon_scaled[:, num_features:]\n",
    "\n",
    "print(\"Reversing scaling to get real feature values...\")\n",
    "\n",
    "recon_years = feature_scaler.inverse_transform(recon_num).flatten()\n",
    "recon_states = onehot_encoder.inverse_transform(recon_cat).flatten()\n",
    "\n",
    "recon_df = pd.DataFrame({\n",
    "    \"Year\": np.round(recon_years).astype(int),\n",
    "    \"State Name\": recon_states\n",
    "})\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "save_path = f\"artifacts/reconstructed_real_values{FILE_SUFFIX}.csv\"\n",
    "recon_df.to_csv(save_path, index=False)\n",
    "print(f\"Reconstructed feature values saved to {save_path}\")\n",
    "\n",
    "print(\"Showing reconstructed sensitive features (State Name, Year)...\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(range(len(recon_df)), recon_df[\"Year\"], c=\"red\", label=\"Reconstructed Year\", alpha=0.7)\n",
    "plt.title(\"Recovered Years from Model Inversion Attack\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Year\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSample of reverse-engineered sensitive data:\")\n",
    "print(recon_df.head(10))\n",
    "\n",
    "print(\"\\n Done — reconstructed original 'State Name' and 'Year' from model outputs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
